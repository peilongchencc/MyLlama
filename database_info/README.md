# 大模型数据集基础

- [大模型数据集基础](#大模型数据集基础)
  - [什么是 "SFT"？](#什么是-sft)
  - [什么是 "DPO"？](#什么是-dpo)
    - [DPO举例说明:](#dpo举例说明)
  - [执行步骤:](#执行步骤)
    - [实际操作中的DPO：](#实际操作中的dpo)
    - [总结：](#总结)
  - ["Alpaca格式" 和 "ShareGPT格式":](#alpaca格式-和-sharegpt格式)
    - [**Alpaca格式** 数据示例:](#alpaca格式-数据示例)
    - [**ShareGPT格式** 数据示例:](#sharegpt格式-数据示例)
    - [**Alpaca格式** 中关于 input 的使用:](#alpaca格式-中关于-input-的使用)
      - [1. **`input` 为空的情况**](#1-input-为空的情况)
      - [2. **`input` 有内容的情况**](#2-input-有内容的情况)
      - [总结](#总结-1)


## 什么是 "SFT"？

在大模型领域，"SFT" 通常指的是 "Supervised Fine-Tuning"（监督微调）。这是指在预训练模型（如GPT-3、BERT等）之后，通过有标签的数据进行进一步的训练，使模型能够在特定任务上表现得更好。<br>

具体来说，监督微调包括以下几个步骤：<br>

1. **预训练**：首先，模型在大量无标签的数据上进行预训练，学习语言的基础知识和结构。
2. **微调数据集**：准备一个带标签的数据集，这个数据集包含了特定任务的输入和期望输出。
3. **监督微调**：使用这个带标签的数据集对预训练模型进行微调，通过监督学习的方式优化模型参数，使其在特定任务上表现更好。

SFT在提高模型性能、适应特定任务和领域方面非常有效，广泛应用于自然语言处理（NLP）的各种任务，如文本分类、问答系统、机器翻译等。<br>


## 什么是 "DPO"？

在大模型领域，"DPO" 通常指的是 "Direct Preference Optimization"（直接偏好优化）。这是在训练和微调大模型时，用于优化模型输出以更好地满足用户偏好的一种方法。与传统的监督学习不同，DPO利用用户的反馈直接指导模型的改进。<br>

具体来说，DPO包括以下几个步骤：<br>

1. **收集偏好数据**：从用户那里收集关于模型输出的偏好数据。这些数据可以是用户对不同模型输出的评分或选择，表示哪些输出更符合他们的需求和期望。
2. **建模偏好**：使用收集到的偏好数据来建模用户的偏好，这可以通过各种方法实现，包括回归、排序模型等。
3. **优化模型**：根据用户的偏好对模型进行优化，使其输出更符合用户的期望。这可能涉及调整模型参数、改变生成策略或使用偏好数据进行微调。

通过DPO，模型可以更好地理解和满足用户的需求，提高用户体验和满意度。这种方法在个性化推荐系统、聊天机器人和其他需要考虑用户偏好的应用中尤为重要。<br>

### DPO举例说明:

假设我们有一个聊天机器人，用户可以用它来获取旅游建议。为了让聊天机器人提供更好的建议，我们希望优化它的输出，使其更符合用户的偏好。DPO执行步骤如下:<br>

## 执行步骤:

1. **收集偏好数据**：
   - 用户与聊天机器人交互，获取旅游建议。
   - 每次建议后，用户可以对建议进行评分（例如1到5星）或直接选择他们更喜欢的建议（例如从两个选项中选择一个）。

   示例对话：
   - 用户：我想去一个有海滩的地方度假。
   - 聊天机器人：你可以考虑去夏威夷或马尔代夫。
   - 用户选择：我更喜欢马尔代夫。

2. **建模偏好**：
   - 收集一段时间后，我们有大量关于用户偏好的数据（比如，用户更喜欢哪种类型的目的地）。
   - 使用这些数据来训练一个偏好模型，这个模型可以预测用户在类似情况下会更喜欢哪种建议。

3. **优化模型**：
   - 根据偏好模型的预测结果，对聊天机器人的生成策略进行调整。
   - 比如，如果偏好模型显示大多数用户更喜欢热带海滩而不是寒冷的山地，那么聊天机器人在生成旅游建议时会优先推荐热带海滩。

### 实际操作中的DPO：

信息收集一段时间后，假设我们有以下多组用户反馈，用户对不同旅游建议的选择记录如下：<br>

- **用户1**：夏威夷（喜欢） vs 马尔代夫（不喜欢）
- **用户2**：马尔代夫（喜欢） vs 巴哈马（不喜欢）
- **用户3**：巴厘岛（喜欢） vs 夏威夷（不喜欢）
- **用户4**：夏威夷（不喜欢） vs 马尔代夫（喜欢）
- **用户5**：巴厘岛（喜欢） vs 马尔代夫（不喜欢）

通过计算每个地方被喜欢的次数：<br>

- **夏威夷**：喜欢 1 次（用户1）
- **马尔代夫**：喜欢 2 次（用户2、用户4）
- **巴哈马**：喜欢 0 次
- **巴厘岛**：喜欢 2 次（用户3、用户5）

通过这些数据，我们可以总结出用户更喜欢 **马尔代夫** 和 **巴厘岛** 这样的热带海滩胜过其他地方。于是，我们调整聊天机器人的生成策略：<br>

   - 用户：我想去一个有海滩的地方度假。
   - 优化后的聊天机器人：你可以考虑去马尔代夫或巴厘岛。

### 总结：

通过DPO，我们利用用户的实际偏好数据来优化聊天机器人的推荐系统，使其能够提供更符合大多数用户期望的建议。DPO的关键在于收集用户偏好、建模这些偏好，并据此调整和优化模型的输出，使其更贴合用户需求。<br>


## "Alpaca格式" 和 "ShareGPT格式":

在大模型训练的数据集中，"Alpaca格式" 和 "ShareGPT格式" 指的是特定的对话数据结构，用于训练模型以生成对话或回答问题。具体来说：<br>

1. **Alpaca格式**：
   - **Alpaca** 是一个衍生自 Meta 的 LLaMA 模型的项目，由斯坦福大学团队开发。Alpaca 使用了一种特定的数据格式来组织训练数据，这些数据通常包括一个提示（prompt）和模型生成的响应（response）。这种格式的结构简单，通常如下：
     - **指令（Instruction）**：用户给模型的指令或问题。
     - **输入（Input）**：该字段可以为空，或者包含一些额外的信息来帮助模型生成更好的回答。
     - **输出（Output）**：模型根据指令生成的回答或响应。

   这个格式的关键在于通过指令和响应对来教模型如何处理特定类型的输入，并生成对应的输出。

2. **ShareGPT格式**：
   - **ShareGPT** 是一个与 ChatGPT 相关的社区平台，用户可以在该平台上分享与 ChatGPT 的对话记录。ShareGPT格式的数据集通常包含完整的对话记录，记录了人类和模型之间的交互。这种格式的特点是保留了对话的上下文，而不仅仅是单独的问答对。ShareGPT格式可能包括：
     - **对话轮次（Turns）**：由人类和模型交替发言的完整对话记录。每一轮可能包括：
       - **用户输入（User Input）**：用户给出的每一个输入。
       - **模型响应（Model Response）**：模型对该输入的响应。

   这种格式适用于训练模型理解和参与更复杂的、多轮的对话。

总结来说，**Alpaca格式**侧重于单一指令和响应对，而**ShareGPT格式**则保留了完整的对话上下文，用于更复杂的对话训练。

### **Alpaca格式** 数据示例:

Alpaca格式通常用于简单的 **指令-响应对** 。这种格式包括三个主要部分：`instruction`（指令）、`input`（输入，可选）和`output`（输出）。

**示例：**

```json
{
  "instruction": "解释什么是机器学习。",
  "input": "",
  "output": "机器学习是一种人工智能技术，它使计算机系统能够从数据中学习并根据所学的模式做出决策或预测，而无需显式编程。"
}
```

**带输入的示例：**

```json
{
  "instruction": "总结以下段落。",
  "input": "机器学习是人工智能的一个子领域，涉及从数据中自动检测模式，并根据这些模式进行预测或决策，而无需明确的规则编程。",
  "output": "机器学习是一种无需显式编程，通过数据检测模式并进行预测的AI技术。"
}
```

### **ShareGPT格式** 数据示例:

ShareGPT格式用于记录完整的对话流，包括用户和模型的多轮交互。

**示例：**

```json
{
  "conversation": [
    {
      "role": "user",
      "content": "什么是机器学习？"
    },
    {
      "role": "assistant",
      "content": "机器学习是一种人工智能技术，它使计算机系统能够从数据中学习并根据所学的模式做出决策或预测，而无需显式编程。"
    },
    {
      "role": "user",
      "content": "能给个例子吗？"
    },
    {
      "role": "assistant",
      "content": "当然！例如，电子邮件中的垃圾邮件过滤器就是通过机器学习来工作的。它会分析大量垃圾邮件和非垃圾邮件的特征，学习如何区分它们，并根据这些特征来过滤新邮件。"
    }
  ]
}
```

在ShareGPT格式中，每个 `conversation` 数组中的对象表示一次对话轮次，由 `role`（角色：`user` 或 `assistant`）和 `content`（内容）组成。这种格式保留了对话的上下文，有助于训练模型在复杂对话中理解和生成更相关的响应。

### **Alpaca格式** 中关于 input 的使用: 

在 **Alpaca格式** 中，`input` 字段是否需要填写，取决于指令的类型和任务的需求。

#### 1. **`input` 为空的情况**

当指令本身已经包含了所有必要的信息时，`input` 字段可以为空。这种情况下，模型只需要根据 `instruction` 来生成 `output`，无需额外的上下文或数据。

**示例：**

- **指令：** "解释什么是机器学习。"  
  - 这里，`instruction` 已经足够明确，模型只需要回答这个问题即可，`input` 不需要额外提供信息。

```json
{
  "instruction": "解释什么是机器学习。",
  "input": "",
  "output": "机器学习是一种人工智能技术，它使计算机系统能够从数据中学习并根据所学的模式做出决策或预测，而无需显式编程。"
}
```

#### 2. **`input` 有内容的情况**

当指令要求对特定的内容进行操作或加工时，`input` 字段就会被用来提供这些内容。此时，`instruction` 描述的是要对 `input` 进行什么操作，而 `input` 则提供了操作的具体对象。

**示例：**

- **指令：** "总结以下段落。"  
  - 在这个例子中，`instruction` 是让模型总结某个段落，但段落内容在 `instruction` 中没有明确给出，因此需要在 `input` 字段中提供。

```json
{
  "instruction": "总结以下段落。",
  "input": "机器学习是人工智能的一个子领域，涉及从数据中自动检测模式，并根据这些模式进行预测或决策，而无需明确的规则编程。",
  "output": "机器学习是一种无需显式编程，通过数据检测模式并进行预测的AI技术。"
}
```

#### 总结
- **`input` 为空：** 当 `instruction` 已经足够明确，不需要额外的上下文或具体内容来执行指令时，`input` 为空。
- **`input` 有内容：** 当 `instruction` 需要针对某个具体内容进行操作（如总结、翻译、回答基于上下文的问题等）时，`input` 中需要填写这个具体内容。
# 模型训练相关基础

## LoRA微调后，合并参数在预测之后，还是预测之前？

这主要取决于你预测(推理)的方式:

如果你采用的是使用的是一个单独的 LoRA 适配器，并没有将 LoRA 参数直接合并到基础模型中。

这意味着在推理过程中，模型将基础模型与 LoRA 适配器结合使用，而不是将 LoRA 参数合并到模型权重中并生成一个新的模型文件‼️‼️

这种方式的优点是灵活性高，因为你可以在不同的任务或场景中使用不同的适配器，而无需每次都重新训练或保存一个新的完整模型。

🚨缺点：每次推理时都需要加载基础模型和适配器，这可能会影响推理的速度或增加显存的需求，尽管通常相对快速，但还是会有一些影响。